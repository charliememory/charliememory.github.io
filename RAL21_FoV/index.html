<!DOCTYPE html> 
<html>
<head>
  <title>FoV-Net: Field-of-View Extrapolation Using Self-Attention and Uncertainty</title>
  <link rel="stylesheet" href="./project.css?v={random number/string}">
  <!-- <link rel="stylesheet" href="../ThirdParty/font-awesome/css/font-awesome.min.css"> -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
  <script language="JavaScript">
    function ShowHide(divId) {
      if (document.getElementById(divId).style.display == 'none') {
        document.getElementById(divId).style.display = 'block';
      } else {
        document.getElementById(divId).style.display = 'none';
      }
    }
    </script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>

  <meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate" />
  <meta http-equiv="Pragma" content="no-cache" />
  <meta http-equiv="Expires" content="0" />
</head>
<body>

  <div id="footer"/></div>

    <center>
    <h1>FoV-Net: Field-of-View Extrapolation Using Self-Attention and Uncertainty</h1>
    <span class="Authors"><a href="http://charliememory.github.io/" target="_blank"><b>Liqian Ma<sup>1</sup>,&nbsp;</b></a></span>
    <span class="Authors"><a href="https://people.ee.ethz.ch/~georgous/" target="_blank"><b>Stamatios Georgoulis<sup>2</sup>,&nbsp;</b></a></span>
    <span class="Authors"><a href="https://stephenjia.github.io/" target="_blank"><b>Xu Jia<sup>3</sup>,&nbsp;</b></a></span>
    <span class="Authors"><a href="http://www.vision.ee.ethz.ch/~vangool/" target="_blank"><b>Luc Van Gool<sup>1,2</sup>,&nbsp;</b></a></span>

    <p><sup>1</sup>KU Leuven &nbsp; <sup>2</sup> ETH Zurich &nbsp; <sup>3</sup> Dalian University of Technology</p>
    <p><b>IEEE Robotics and Automation Letters 2021</b></p>
    </center>

    <center>
    <div class="icons">
      <span class="MaterialsForDL"><a href="https://ieeexplore.ieee.org/document/9384202" target="_blank"> <i class="fa fa-file-pdf-o fa-2x"></i>&nbsp; Paper &nbsp;</a></span>
      <span class="MaterialsForDL"><a href="../pdf/RAL_ICRA21_FoV_Extrapolation.pdf" target="_blank"> <i class="fa fa-file-pdf-o fa-2x"></i>&nbsp; PDF &nbsp;</a></span>
<!--       <span class="MaterialsForDL"><a href="ECCV20_Unselfie_supp.pdf" target="_blank"> <i class="fa fa-file-pdf-o fa-2x"></i>&nbsp;Supp&nbsp;</a></span> -->
<!--       <span class="MaterialsForDL"><a href="https://arxiv.org/abs/2007.15068" target="_blank"> <i class="fa fa-link fa-2x"></i>&nbsp;(TODO) ArXiv&nbsp;</a></span> -->
      <span class="MaterialsForDL"> <i class="fa fa-github fa-2x">
      <span class="MaterialsForDL"><a href="../bibtex/RAL_ICRA21_Bibtex.txt" target="_blank"> <i class="fa fa-file-text-o fa-2x"></i>&nbsp;Bibtex&nbsp;</a></span>
<!--       <span class="MaterialsForDL"><a href="./ECCV20_Unselfie_slides.pdf" target="_blank"> <i class="fa fa-slideshare fa-2x"></i>&nbsp;(TODO) Slides&nbsp;</a></span> -->
    </div>
<!-- 
    <div class="bib" id="bibECCV20Unselfie" style="DISPLAY: none">
      <p style="text-align:left;">
      <b>Bibtex</b>
      <br>@inproceedings{ma2020unselfie,
      <br> author = {Ma, Liqian and Lin, Zhe and Barnes, Connelly and Efros, Alexei A and Lu, Jingwan},
      <br> title = {Unselfie: Translating Selfies to Neutral-pose Portraits in the Wild},
      <br> booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
      <br> month = {August},
      <br> year = {2020}
      <br>} 
      </p>
    </div> -->
    </center>
 <hr>
    <!-- <img src="./misc/teaser.jpg" title="Teaser" style="width:808px; height: 386px;" align="middle"> -->

  <div id="Abstract" style="margin-left: auto; margin-right: auto; width: 1000px; text-align:justify;">
    <center>
      <h2 >Abstract</h2>
    </center>
    <p>The ability to make educated predictions about their surroundings, and associate them with certain confidence, is important for intelligent systems, like autonomous vehicles and robots. It allows them to plan early and decide accordingly. Motivated by this observation, in this paper we utilize information from a video sequence with a narrow field-of-view to infer the scene at a wider field-of-view. To this end, we propose a temporally consistent field-of-view extrapolation framework, namely FoV-Net, that: (1) leverages 3D information to propagate the observed scene parts from past frames; (2) aggregates the propagated multi-frame information using an attention-based feature aggregation module and a gated self-attention module, simultaneously hallucinating any unobserved scene parts; and (3) assigns an interpretable uncertainty value at each pixel. Extensive experiments show that FoV-Net does not only extrapolate the temporally consistent wide field-of-view scene better than existing alternatives, but also provides the associated uncertainty which may benefit critical decision-making downstream applications.</p>
<!--     <p>Generating novel, yet realistic, images of persons is a challenging task due to the complex interplay between the different image factors, such as the foreground, background and pose information. In this work, we aim at generating such images based on a novel, two-stage reconstruction pipeline that learns a disentangled representation of the aforementioned image factors and generates novel person images at the same time. First, a multi-branched reconstruction network is proposed to disentangle and encode the three factors into embedding features, which are then combined to re-compose the input image itself. Second, three corresponding mapping functions are learned in an adversarial manner in order to map Gaussian noise to the learned embedding feature space, for each factor, respectively. Using the proposed framework, we can manipulate the foreground, background and pose of the input image, and also sample new embedding features to generate such targeted manipulations, that provide more control over the generation process. Experiments on the Market-1501 and Deepfashion datasets show that our model does not only generate realistic person images with new foregrounds, backgrounds and poses, but also manipulates the generated factors and interpolates the in-between states. Another set of experiments on Market-1501 shows that our model can also be beneficial for the person re-identification task.</p> -->
<!-- 
    <p>Motivation:
    make the selfie photo look like a well-composed portrait captured by a third-party photographer.
    </p>
    <p>Task:
    unselfie, a novel photographic translation, that automatically translates a selfie into a neutral-pose portrait.
    </p>
    <p>Challenges:
    1) Lack of paired selfie-portrait data.
    2) Multi-modal results with different target neutral poses.
    3) Dis-occlusion and seamless composition.
    </p>
    <p>Key idea:
    1) Search the suitable neutral body pose. 2) Inpaint the body texture in coordinate space with synthesized pair data. 3) Refine body texture, fill in background holes, and compose body and background seamlessly.
    </p>
    <p>Contributions:
    <ul>
      <li>We introduce the unselfie problem, collect an unpaired dataset, and introduce a way to synthesize paired training data for self-supervised learning.</li>
      <li>We introduce a three-stage pipeline to translate selfies into neutral-pose portraits.</li>
      <li>We propose a novel nearest pose search module to obtain suitable multi-modal target neutral poses.</li>
      <li>We design an all-in-one composition module to refine the foreground, complete the background, and compose them together seamlessly.</li>
    </ul>
    </p> -->

    <div id="Teaser" style="margin-left: auto; margin-right: auto; width: 1000px; text-align:justify;">
      <center>
      <img src="./imgs/teaser.gif" title="teaser" style="width:1000px;">
      </center>
    </div>

<!--     <center>
      <h2 >(TODO) Video.</h2>
    </center> -->

  </div>
  <hr>
<!-- 
  <hr>
  <div id="Resources" style="margin-left: auto; margin-right: auto; width: 1000px;">
    <h2 >Resources</h2>
    <div class="icons">
      <span class="MaterialsForDL"><a href="../pdf/ECCV20_Unselfie.pdf" target="_blank"> <i class="fa fa-file-pdf-o fa-2x"></i>&nbsp;Paper&nbsp;</a></span>
      <span class="MaterialsForDL"><a href="https://arxiv.org/abs/2007.15068" target="_blank"> <i class="fa fa-link fa-2x"></i>&nbsp;ArXiv&nbsp;</a></span>
      <span class="MaterialsForDL"> <i class="fa fa-github fa-2x"></i>&nbsp;Code[come soon]&nbsp;</span>
      <span class="MaterialsForDL"> <i class="fa fa-database fa-2x"></i>&nbsp;Dataset[come soon]&nbsp;</span>
      <span class="MaterialsForDL"><a onclick="javascript:ShowHide('bibCVPR18DPIG')" href="javascript:;"><i class="fa fa-file-text-o fa-2x"></i>&nbsp;Bibtex&nbsp;</a></span>
      <span class="MaterialsForDL"><a href="./ECCV20_Unselfie_slides.pdf" target="_blank"> <i class="fa fa-slideshare fa-2x"></i>&nbsp;Slides&nbsp;</a></span>
    </div>

    <div class="bib" id="bibECCV20Unselfie" style="DISPLAY: none">
      <b>Bibtex</b>
      <br>@inproceedings{ma2020unselfie,
      <br> author = {Ma, Liqian and Lin, Zhe and Barnes, Connelly and Efros, Alexei A and Lu, Jingwan},
      <br> title = {Unselfie: Translating Selfies to Neutral-pose Portraits in the Wild},
      <br> booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
      <br> month = {August},
      <br> year = {2020}
      <br>} 
    </div>
  </div>
  <hr> -->


<div id="Framework" style="margin-left: auto; margin-right: auto; width: 1000px; text-align:justify;">
  <center>
    <h2 >Framework</h2>
  </center>
  <p> The proposed FoV-Net framework. Left: the coordinate generation stage, which estimates the scene-level rigid flow, governed by the camera motion, and uses it to calculate the coordinates (i.e., pixel displacements). Right: the frame aggregation stage, which utilizes the generated coordinates to propagate past frames information on a multi-scale feature level -- denoted as red dots -- and then aggregates the propagated features with an Attention-based Feature Aggregation module. To synthesize the final result <em>O<sub>t</sub></em>, a U-Net architecture is adopted to in/out-paint the missing regions, where a Gated Self-Attention module is introduced to handle different ambiguities for better generation quality. Concurrently, an uncertainty map <em>U<sub>t</sub></em> is jointly estimated to interpret the hallucination uncertainty at each pixel and guide the learning by reducing supervision ambiguity. Due to FoV-Net's recurrent nature, note that previous outputs become future inputs for temporal coherency purposes.</p>
  <center>
  <img src="./imgs/framework1.svg" title="Pipeline" style="width:800px;">
  </center>

  <center>
    <h2 >Ambiguity illustration</h2>
  </center>
  <p> There is ambiguity existing between the narrow FoV observations and the wide FoV ground truth. The ambiguity mainly comes from two sources: the unobserved information and possible 3D estimation errors. In particular, the pixels in the wide FoV frame can be roughly divided into four types: (a) the observed narrow FoV pixels in the present frame (no ambiguity); (b) the propagated pixels from past frames with accurate propagation (low ambiguity); (c) the propagated pixels from past frames with noisy propagation (medium ambiguity); (d) the unobserved regions (high ambiguity). When the ambiguity is high, strong enforcement of reconstruction losses in those pixels may mislead the training process. In contrast, perceptual and adversarial losses can be more suitable to predict a plausible outcome.</p>
  <center>
  <img src="./imgs/ambiguity5.svg" title="Pipeline" style="width:800px;">
  </center>
</div>
<hr>

<div id="Results" style="margin-left: auto; margin-right: auto; width: 1000px; text-align:justify;">
  <center>
    <h2 >Result Video.</h2>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/wfi-Ays5Xgo" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
  </center>

  <center>
    <h2 >Uncertainty comparisons</h2>
  <!-- </center> -->
  <img src="./imgs/sparsification_cs_v2.svg" title="result" style="width:400px;">
  <img src="./imgs/sparsification_kitti_v2.svg" title="result" style="width:400px;">
  <!-- </center> -->
  <p>The sparsification plots. The x-axis denotes the fraction of removed pixels, and the y-axis shows the MSE on the remaining pixels. MSE converges to 0 after removing all pixels in the out-of-view regions.</p>
  <!-- <center> -->
  </center>

  <center>
    <h2 >Extended applications and failures</h2>
  <!-- </center> -->
  <img src="./imgs/kittiDepth_csSeg_failure.svg" title="result" style="width:800px;">
  <!-- </center> -->
  <p>Left: depth extrapolation. Middle: semantic segmentation extrapolation. Right: failure case.</p>
  <!-- <center> -->
  </center>

  <p>References:<br/>
  [1] Godard, C., et al. Digging into self-supervised monocular depth estimation. In ICCV, 2019. <br/>
  [2] Zhao, H., et al. Exploring Self-attention for Image Recognition In CVPR, 2020. <br/>
  [3] Poggi, M., et al. On the uncertainty of self-supervised monocular depth estimation. In CVPR, 2020. <br/>
  </p>
</div>


<!--   <hr>
  <div id="Spotlight" style="margin-left: auto; margin-right: auto; width: 1000px;">
    <h2 >Spotlight</h2>
    <center>
    <video width="900" height="450" controls>
      <source src="./1801_with_audio_v2.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>
    </center>
  </div>
  <hr> -->

  <div id="footer"/></div>
</body>

<html>
