<!DOCTYPE html> 
<html>
<head>
  <title>Direct Dense Pose Estimation</title>
  <link rel="stylesheet" href="./project.css?v={random number/string}">
  <!-- <link rel="stylesheet" href="../ThirdParty/font-awesome/css/font-awesome.min.css"> -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
  <script language="JavaScript">
    function ShowHide(divId) {
      if (document.getElementById(divId).style.display == 'none') {
        document.getElementById(divId).style.display = 'block';
      } else {
        document.getElementById(divId).style.display = 'none';
      }
    }
    </script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>

  <meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate" />
  <meta http-equiv="Pragma" content="no-cache" />
  <meta http-equiv="Expires" content="0" />
</head>
<body>

  <div id="footer"/></div>

    <center>
    <h1>Direct Dense Pose Estimation</h1>
    <span class="Authors"><a href="http://charliememory.github.io/" target="_blank"><b>Liqian Ma<sup>1</sup>,&nbsp;</b></a></span>
    <span class="Authors"><a href="https://lingjie0206.github.io/" target="_blank"><b>Lingjie Liu<sup>2</sup>,&nbsp;</b></a></span>
    <span class="Authors"><a href="https://people.mpi-inf.mpg.de/~theobalt/" target="_blank"><b>Christian Theobalt<sup>2</sup>,&nbsp;</b></a></span>
    <span class="Authors"><a href="http://www.vision.ee.ethz.ch/~vangool/" target="_blank"><b>Luc Van Gool<sup>1,3</sup></b></a></span>

    <p><sup>1</sup>KU Leuven &nbsp; <sup>2</sup> Max Planck Institute for Informatics &nbsp; <sup>3</sup> ETH Zurich</p>
    <p><b>IEEE International Conference on 3D Vision (3DV) 2021</b></p>
    </center>

    <center>
    <div class="icons">
      <span class="MaterialsForDL"><a href="https://ieeexplore.ieee.org/document/9665828" target="_blank"> <i class="fa fa-file-pdf-o fa-2x"></i>&nbsp; Paper &nbsp;</a></span>
      <span class="MaterialsForDL"><a href="../pdf/3DV2021_DirectDensePose_camera_ready.pdf" target="_blank"> <i class="fa fa-file-pdf-o fa-2x"></i>&nbsp; PDF &nbsp;</a></span>
<!--       <span class="MaterialsForDL"><a href="ECCV20_Unselfie_supp.pdf" target="_blank"> <i class="fa fa-file-pdf-o fa-2x"></i>&nbsp;Supp&nbsp;</a></span> -->
<!--       <span class="MaterialsForDL"><a href="https://arxiv.org/abs/2007.15068" target="_blank"> <i class="fa fa-link fa-2x"></i>&nbsp;(TODO) ArXiv&nbsp;</a></span> -->
      <!-- <span class="MaterialsForDL"> <i class="fa fa-github fa-2x"></i>&nbsp;Code[come soon]&nbsp;</span> -->
      <span class="MaterialsForDL"><a href="../bibtex/3DV21_DDP_Bibtex.txt" target="_blank"> <i class="fa fa-file-text-o fa-2x"></i>&nbsp;Bibtex&nbsp;</a></span>
<!--       <span class="MaterialsForDL"><a href="./ECCV20_Unselfie_slides.pdf" target="_blank"> <i class="fa fa-slideshare fa-2x"></i>&nbsp;(TODO) Slides&nbsp;</a></span> -->
    </div>
<!-- 
    <div class="bib" id="bibECCV20Unselfie" style="DISPLAY: none">
      <p style="text-align:left;">
      <b>Bibtex</b>
      <br>@inproceedings{ma2020unselfie,
      <br> author = {Ma, Liqian and Lin, Zhe and Barnes, Connelly and Efros, Alexei A and Lu, Jingwan},
      <br> title = {Unselfie: Translating Selfies to Neutral-pose Portraits in the Wild},
      <br> booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
      <br> month = {August},
      <br> year = {2020}
      <br>} 
      </p>
    </div> -->
    </center>
 <hr>
    <!-- <img src="./misc/teaser.jpg" title="Teaser" style="width:808px; height: 386px;" align="middle"> -->

  <div id="Abstract" style="margin-left: auto; margin-right: auto; width: 1000px; text-align:justify;">
    <center>
      <h2 >Abstract</h2>
    </center>
    <p>Dense human pose estimation is the problem of learning dense correspondences between RGB images and the surfaces of human bodies, which finds various applications, such as human body reconstruction, human pose transfer, and human action recognition. Prior dense pose estimation methods are all based on Mask R-CNN framework and operate in a top-down manner of first attempting to identify a bounding box for each person and matching dense correspondences in each bounding box. Consequently, these methods lack robustness due to their critical dependence on the Mask R-CNN detection, and the runtime increases drastically as the number of persons in the image increases. We therefore propose a novel alternative method for solving the dense pose estimation problem, called Direct Dense Pose (DDP). DDP first predicts the instance mask and global IUV representation separately and then combines them together. We also propose a simple yet effective 2D temporal-smoothing scheme to alleviate the temporal jitters when dealing with video data. Experiments demonstrate that DDP overcomes the limitations of previous top-down baseline methods and achieves competitive accuracy. In addition, DDP is computationally more efficient than previous dense pose estimation methods, and it reduces jitters when applied to a video sequence, which is a problem plaguing the previous methods.</p>


    <div id="Teaser" style="margin-left: auto; margin-right: auto; width: 1000px; text-align:justify;">
      <center>
      <img src="./imgs/result_vis.svg" title="teaser" style="width:1000px;">
      </center>
    </div>


    <center>
      <h2 >Video (1 min). Full video is <a href="https://slideslive.com/38972323/direct-dense-pose-estimation?ref=account-96722-latest" target="_blank"> &nbsp;here&nbsp;</a></h2>
<!--       <iframe width="700" height="394" src="https://www.youtube.com/embed/JIlDwqfxFAA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe> -->
      <iframe width="800" height="450"  src="https://www.youtube.com/embed/JIlDwqfxFAA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
    </center>

<!--   10min https://slideslive.com/38972323/direct-dense-pose-estimation?ref=account-96722-latest
  2min https://slideslive.com/38972182/direct-dense-pose-estimation?ref=account-96722-latest -->

  </div>
  <hr>
<!-- 
  <hr>
  <div id="Resources" style="margin-left: auto; margin-right: auto; width: 1000px;">
    <h2 >Resources</h2>
    <div class="icons">
      <span class="MaterialsForDL"><a href="../pdf/ECCV20_Unselfie.pdf" target="_blank"> <i class="fa fa-file-pdf-o fa-2x"></i>&nbsp;Paper&nbsp;</a></span>
      <span class="MaterialsForDL"><a href="https://arxiv.org/abs/2007.15068" target="_blank"> <i class="fa fa-link fa-2x"></i>&nbsp;ArXiv&nbsp;</a></span>
      <span class="MaterialsForDL"> <i class="fa fa-github fa-2x"></i>&nbsp;Code[come soon]&nbsp;</span>
      <span class="MaterialsForDL"> <i class="fa fa-database fa-2x"></i>&nbsp;Dataset[come soon]&nbsp;</span>
      <span class="MaterialsForDL"><a onclick="javascript:ShowHide('bibCVPR18DPIG')" href="javascript:;"><i class="fa fa-file-text-o fa-2x"></i>&nbsp;Bibtex&nbsp;</a></span>
      <span class="MaterialsForDL"><a href="./ECCV20_Unselfie_slides.pdf" target="_blank"> <i class="fa fa-slideshare fa-2x"></i>&nbsp;Slides&nbsp;</a></span>
    </div>

    <div class="bib" id="bibECCV20Unselfie" style="DISPLAY: none">
      <b>Bibtex</b>
      <br>@inproceedings{ma2020unselfie,
      <br> author = {Ma, Liqian and Lin, Zhe and Barnes, Connelly and Efros, Alexei A and Lu, Jingwan},
      <br> title = {Unselfie: Translating Selfies to Neutral-pose Portraits in the Wild},
      <br> booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
      <br> month = {August},
      <br> year = {2020}
      <br>} 
    </div>
  </div>
  <hr> -->


<div id="Framework" style="margin-left: auto; margin-right: auto; width: 1000px; text-align:justify;">
  <center>
    <h2 >Framework</h2>
  </center>
  <p> The ResNet-FPN based backbone is first used to extract a feature pyramid. Then, the feature aggregation module is applied to aggregate the feature pyramid into a global feature representation. Such a global feature is then fed into the instance branch and global IUV branch, respectively, to estimate the instance-level masks and the global IUV representation. Note that, for each instance, the Mask FCN weights are generated dynamically via a weight generator.</p>
  <center>
  <img src="./imgs/framework.svg" title="Pipeline" style="width:1000px;">
  </center>

  <p> Applying image-based methods to video data directlyleads to the undesirable flickering issue, since the temporal information is not considered. To address this, we introduce a simple and effective 2D temporal-smoothing scheme.</p>
  <center>
  <iframe  width="800" height="450" src="https://www.youtube.com/embed/N1rogCFUNSE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
  </center>

</div>
<hr>

<div id="Results" style="margin-left: auto; margin-right: auto; width: 1000px; text-align:justify;">

  <center>
    <h2 >Comparisons</h2>
    <!-- </center> -->
    <p> Top-down methods (e.g. [1]) suffer from early commitment (part missing <span style="property:red;">red</span>) and heavy compression (inaccurate boundary <span style="property:blue;">blue</span>).</p>
    <img src="./imgs/result_vis2.svg" title="result" style="width:800px;">
    <p>The quantitative results on DensePose-COCO minival split. * Models are trained with a simulated dataset.</p>
    <img src="./imgs/table.svg" title="table" style="width:800px;">
  </center>



</div>
<hr>

<div id="Application" style="margin-left: auto; margin-right: auto; width: 1000px; text-align:justify;">

  <center>
    <h2 >Application on image-to-image translation</h2>
  </center>
  <!-- </center> -->
  <p>We also apply our method on the human pose transfer task, i.e. translate the dense pose IUV representation input to an RGB image. We adopt the popular pix2pixHD [2] as our translation model and generate the image frame-by-frame. We observe that our temporally smooth dense pose can help alleviate video flickering issue and generate stable video results.</p>
  <center>
    <!-- <img src="./imgs/I2I_video.gif" title="I2I_video" style="width:800px;"> -->
    <iframe  width="800" height="450" src="https://www.youtube.com/embed/4DydJJANIBM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

  </center>

</div>
<hr>


<div id="Conclusions" style="margin-left: auto; margin-right: auto; width: 1000px; text-align:justify;">

  <center>
    <h2 >Advantages</h2>
  </center>
  <ul>
    <li>Avoiding issues in top-down framework including early commitment, overlapping ambiguity, heavy compression and computation.</li>
    <li>Producing temporal smoothing results on video data.</li>
  </ul>
  <!-- <hr> -->

  <center>
    <h2 >Limitations</h2>
    <img src="./imgs/limitations.svg" title="limitations" style="width:1000px;">
  </center>
  <ul>
    <li> Sensitive to scale variation and may produce noisy results (red circles).</li>
    <li> Failure to detect some heavily occluded small people (yellow circles).</li>
    <li> Temporal smoothing scheme may fail in textureless region or varying illumination where the optical flow estimation is not robust.</li>
  </ul>

  <p>References:<br/>
  [1] Neverova, Natalia, et al. Continuous surface embeddings. In NeurIPS 2020. <br/>
  [2] Wang, T. C., et al. High-resolution image synthesis and semantic manipulation with conditional gans. In ICCV 2019. <br/>
  </p>
</div>




  <div id="footer"/></div>
</body>

<html>
