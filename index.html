 
<!DOCTYPE html>
<html lang="en"> 
<title>Liqian Ma's Homepage</title>
<!-- <link rel="shortcut icon" href="http://cg.cs.tsinghua.edu.cn/people/~xianying/favicon.ico"> -->
<style type="text/css">
body {
    margin-top: 30px;
    margin-bottom: 30px;
    margin-left: 100px;
    margin-right: 100px;
}
p {
    margin-top: 0px;
    margin-bottom: 0px;
}

.caption {
    font-size: 34px;
    font-weight: normal;
    color: #000;
    font-family: Times New Roman, "Lucida Bright", "DejaVu Serif", Georgia, serif;
}
.caption-1 {
    font-size: 16px;
    font-family: Tahoma, Geneva, sans-serif;
}
.caption-2 {
    font-size: 16px;
    font-family: Tahoma, Geneva, sans-serif;
    font-weight: bold;
    color: #990000;
}
.caption-3 {
    font-size: 16px;
    font-family: Tahoma, Geneva, sans-serif;
    font-weight: bold;
    color: #F00;
}

.caption-4 {
    font-size: 16px;
    font-family: Tahoma, Geneva, sans-serif;
    color: #990000;
}
.content {
    font-size: 16px;
    font-family: Tahoma, Geneva, sans-serif;
    text-align: justify;
}
.content a {
    font-size: 16px;
    font-family: Tahoma, Geneva, sans-serif;
    color: #000099;
}
.content strong a {
    font-size: 16px;
    font-family: Tahoma, Geneva, sans-serif;
    color: #000099;
}
.title-small {
    font-size: 20px;
    font-family: Georgia, "Times New Roman", Times, serif;
    font-weight: bold;
    color: #F90;
}
.title-large {
    font-size: 24px;
    font-family: Georgia, "Times New Roman", Times, serif;
    font-weight: bold;
    color: #000;
}
.margin {
    font-size: 10px;
    line-height: 10px;
}
.margin-small {
    font-size: 5px;
    line-height: 5px;
}
.margin-large {
    font-size: 16px;
    line-height: 16px;
}
#photo {
    box-shadow:6px 6px 4px #888888;
    height:200px; 
}
.teaser-small {
    border-style: solid;
    border-width: 1px;
    width: 200px;
}

a:link {
    text-decoration: none;
}
a:visited {
    text-decoration: none;
}
content a:link {
    text-decoration: none;
}
content a:visited {
    text-decoration: none;
}
a:hover {
    text-decoration: underline;
}
a:active {
    text-decoration: underline;
    color: #000000;
    font-family: Tahoma, Geneva, sans-serif;
}
strong a:active {
    text-decoration: underline;
    color: #000000;
}
img
{
 border-color: black;
}


</style>
<script async="" src="./Liqian Ma&#39;s Homepage_files/analytics.js"></script><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-53682931-1', 'auto');
  ga('send', 'pageview');

</script><style>.ita-icon-0{background-position:-14px -17px;}.ita-icon-1{background-position:-64px -17px;}.ita-icon-2{background-position:-114px -17px;}.ita-icon-3{background-position:-164px -17px;}.ita-icon-4{background-position:-214px -17px;}.ita-icon-5{background-position:-264px -17px;}.ita-icon-6{background-position:-314px -17px;}.ita-icon-7{background-position:-364px -17px;}.ita-icon-8{background-position:-414px -17px;}.ita-icon-9{background-position:-464px -17px;}.ita-icon-10{background-position:-14px -67px;}.ita-icon-11{background-position:-64px -67px;}.ita-icon-12{background-position:-114px -67px;}.ita-icon-13{background-position:-164px -67px;}.ita-icon-14{background-position:-214px -67px;}.ita-icon-15{background-position:-264px -67px;}.ita-icon-16{background-position:-314px -67px;}.ita-icon-17{background-position:-364px -67px;}.ita-icon-18{background-position:-414px -67px;}.ita-icon-19{background-position:-464px -67px;}.ita-icon-20{background-position:-14px -117px;}.ita-icon-21{background-position:-64px -117px;}.ita-icon-22{background-position:-114px -117px;}.ita-icon-23{background-position:-164px -117px;}.ita-icon-24{background-position:-214px -117px;}.ita-icon-25{background-position:-264px -117px;}.ita-icon-26{background-position:-314px -117px;}.ita-icon-27{background-position:-364px -117px;}.ita-icon-28{background-position:-414px -117px;}.ita-icon-29{background-position:-464px -117px;}.ita-icon-30{background-position:-14px -167px;}.ita-icon-31{background-position:-64px -167px;}.ita-icon-32{background-position:-114px -167px;}.ita-icon-33{background-position:-164px -167px;}.ita-icon-34{background-position:-214px -167px;}.ita-icon-35{background-position:-264px -167px;}.ita-icon-36{background-position:-314px -167px;}.ita-icon-37{background-position:-364px -167px;}.ita-icon-38{background-position:-414px -167px;}.ita-icon-39{background-position:-464px -167px;}.ita-icon-40{background-position:-14px -217px;}.ita-icon-41{background-position:-64px -217px;}.ita-icon-42{background-position:-114px -217px;}.ita-icon-43{background-position:-164px -217px;}.ita-icon-44{background-position:-214px -217px;}.ita-icon-45{background-position:-264px -217px;}.ita-icon-46{background-position:-314px -217px;}</style><style id="style-1-cropbar-clipper">/* Copyright 2014 Evernote Corporation. All rights reserved. */
.en-markup-crop-options {
    top: 18px !important;
    left: 50% !important;
    margin-left: -100px !important;
    width: 200px !important;
    border: 2px rgba(255,255,255,.38) solid !important;
    border-radius: 4px !important;
}

.en-markup-crop-options div div:first-of-type {
    margin-left: 0px !important;
}
</style>

<meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="0" />
</head>


<body>

<table border="0" width="100%"><tbody><tr>
    <td width="185"><a href="./Homepage_files/profile_photo.jpg"><img id="photo" class="img-rounded" src="./Homepage_files/profile_photo.jpg"></a></td>
    <td width="15"></td>
    <td></td>
    <td><table border="0" width="100%"><tbody>
        <tr height="40"><td><div class="content">
            <p class="caption">Liqian Ma (马里千)</p>
<!--             <br>Ph.D Candidate
            <br>ESAT-PSI, KU Leuven
            <br>Kasteelpark Arenberg 10 - bus 2441
            <br>B-3001 Heverlee, Belgium
            <br><strong>Email: </strong>Liqian dot Ma at esat dot kuleuven dot be -->

            <br>Algorithm Head
            <br>ZMO AI Inc.
            <br><strong>Email: </strong>LiqianMa dot Scholar at outlook dot com
        </div></td></tr>

        <tr height="20"><td>
            <p class="margin">&nbsp;</p>
            <p class="content"><strong><a href="./Homepage_files/Ma_Liqian_resume.pdf">[CV]</a></strong> 
            &nbsp; <strong><a href="https://github.com/charliememory/">[GitHub]</a></strong> 
            &nbsp; <strong><a href="https://scholar.google.com/citations?user=FTWcemsAAAAJ&hl=en">[Google Scholar]</a></strong>  
            &nbsp; <strong><a href="https://www.semanticscholar.org/author/Liqian-Ma/1847145">[Semantic Scholar]</a></strong>
            &nbsp; <strong><a href="https://www.linkedin.com/in/liqian-ma-68988182/">[Linkedin]</a></strong></p>
        </td></tr>
        <tr height="20">
        <td colspan="2"></td></tr>
    </tbody></table></td>
</tr></tbody></table>


<p class="margin">&nbsp;</p>
<hr align="left">
<div class="title-large">Biography</div>
<p class="content">I am the algorithm head at <a href="https://www.zmo.ai/en" target="_blank">ZMO.AI</a> and lead the R&D team for AIGC. I obtained my PhD degree at <a href="http://www.esat.kuleuven.be/psi" target="_blank">VISICS</a> group of KU Leuven supervised by <a href="http://www.vision.ee.ethz.ch/en/members/get_member.cgi?lang=en&id=1" target="_blank">Prof. Luc Van Gool</a>. 
<!-- I also collaborated with <a href="http://www.vision.ee.ethz.ch/en/members/get_member.cgi?lang=en&id=1" target="_blank">Prof. Luc Van Gool</a>.  -->
Before this, I obtained my Master's and Bachelor's degree from Peking University and South China University of Technology. My research goal is to develop a creative self-learning AI system. Currently, I am working on image/video understanding and synthesis. Please contact me if you are interested in collaboration or internship/fulltime positions.
<!-- filling the gap between data and deep model. --></p>


<!-- <p class="margin">&nbsp;</p>
<div class="title-large">Research Interests</div>
    <div class="content"><ul>
    <li><strong>Visioin and Language</strong>: Image captioning, visual question answering, language grounding with vision</li>
    <li><b>Object recognition</b>: visual tracking, semantic segmentation </li>
    <li><b>Image editing</b>: image generation, video prediction, image super-resolution, image retargeting </li>
    <li><b>Machine Learning</b>: deep learning, sparse coding </li>
    </ul><div> -->


<p class="margin">&nbsp;</p>
</p><p class="title-large">News</p>
<p class="content">[2022.08] Our paper "Controllable Radiance Fields for Dynamic Face Synthesis" is accepted by 3DV 2022.</p>
<p class="content">[2022.08] Our paper "UNIF: United Neural Implicit Functions for Clothed Human Reconstruction and Animation" is accepted by ECCV 2022.</p>
<p class="content">[2021.11] Our paper "Direct Dense Pose Estimation" is accepted by 3DV 2021.</p>
<p class="content">[2021.02] Our paper "FoV-Net: Field-of-View Extrapolation Using Self-Attention and Uncertainty" is accepted by RA-L and ICRA 2021.</p>
<p class="content">[2020.07] Our paper "Unselfie: Translating Selfies to Neutral-pose Portraits in the Wild" is accepted by ECCV 2020.</p>
<p class="content">[2020.07] Our paper "Unpaired Image Shape Translation Across Fashion Data" is accepted by ICIP 2020 (best paper award finalist).</p>
<!-- <p class="content">[2019.06] I worked as a research intern in <a href="https://research.adobe.com/" target="_blank">Adobe Research</a> .</p>
<p class="content">[2019.04] Our paper "A Novel BiLevel Paradigm for Image-to-Image Translation" is available on Arxiv.</p>
<p class="content">[2018.12] Our paper "Exemplar Guided Unsupervised Image-to-Image Translation with Semantic Consistency" is accepted by ICLR 2019.</p>
<p class="content">[2018.12] Our paper "Unsupervised shape transformer for image translation and cross-domain retrieval" is available on Arxiv.</p>
<p class="content">[2018.11] Our paper "Customized Multi-Person Tracker" is accepted by ACCV 2018 (Oral).</p>
<p class="content">[2018.03] Our paper "Disentangled Person Image Generation" is accepted by CVPR 2018 (Spotlight).</p>
<p class="content">[2018.03] Our paper "Natural and Effective Obfuscation by Head Inpainting" is accepted by CVPR 2018.</p>
<p class="content">[2017.09] Our paper "Pose Guided Person Image Generation" is accepted by NIPS 2017.</p> -->
<br>

<!-- <p id="sect-preprints" class="title-large">Preprints</p> -->
<p class="margin">&nbsp;</p>
</p><p class="title-large">Publications</p>

<p class="margin">&nbsp;</p>
<table border="0" class="content">
  <tbody><tr>
    <td width="140"><img src="./Homepage_files/3DV21_teaser.svg" class="teaser-small"></td>
    <td width="20"></td>
    <td valign="middle" width="800"><p><strong>Direct Dense Pose Estimation</strong></p>
        <span><u>Liqian Ma</u>,&nbsp;</span>
        <span><a href="https://lingjie0206.github.io/" target="_blank">Lingjie Liu,&nbsp;</a></span>
        <span><a href="https://people.mpi-inf.mpg.de/~theobalt/" target="_blank">Christian Theobalt,&nbsp;</a></span>
        <span><a href="http://www.vision.ee.ethz.ch/~vangool/" target="_blank">Luc Van Gool&nbsp;</a></span>
        <p>3DV, 2021</p>

        </p><p class="margin-small">&nbsp;</p>
    <!-- <strong><a href="http://charliememory.github.io/RAL_ICRA21_FoV">[Project]</a></strong>&nbsp; -->
    <strong><a href="http://charliememory.github.io/3DV21_DDP/">[Project]</a></strong>&nbsp;
    <!-- <strong><a href="">[Code (come soon)]</a></strong>&nbsp; -->
    <strong><a href="./pdf/3DV2021_DirectDensePose_camera_ready.pdf">[Paper]</a></strong>&nbsp;
    <!-- <strong><a href="">[ArXiv]</a> -->
    </strong>&nbsp;<strong><a href="./bibtex/3DV21_DDP_Bibtex.txt">[BibTex]</a></strong></strong></p>
    </td></tr>
</tbody></table>

<table border="0" class="content">
  <tbody><tr>
    <td width="140"><img src="./Homepage_files/RAL_ICRA21_teaser.png" class="teaser-small"></td>
    <td width="20"></td>
    <td valign="middle" width="800"><p><strong>FoV-Net: Field-of-View Extrapolation Using Self-Attention and Uncertainty</strong></p>
        <span><u>Liqian Ma</u>,&nbsp;</span>
        <span><a href="http://people.ee.ethz.ch/~georgous/" target="_blank">Stamatios Georgoulis,&nbsp;</a></span>
        <span><a href="https://stephenjia.github.io/" target="_blank">Xu Jia,&nbsp;</a></span>
        <span><a href="http://www.vision.ee.ethz.ch/~vangool/" target="_blank">Luc Van Gool&nbsp;</a></span>
        <p>RA-L & ICRA, 2021</p>

        </p><p class="margin-small">&nbsp;</p>
    <!-- <strong><a href="http://charliememory.github.io/RAL_ICRA21_FoV">[Project]</a></strong>&nbsp; -->
    <strong><a href="http://charliememory.github.io/RAL21_FoV/">[Project]</a></strong>&nbsp;
    <!-- <strong>[Code (come soon)]</strong>&nbsp; -->
    <strong><a href="./pdf/RAL_ICRA21_FoV_Extrapolation.pdf">[Paper]</a></strong>&nbsp;
    <!-- <strong><a href="">[ArXiv]</a> -->
    </strong>&nbsp;<strong><a href="./bibtex/RAL_ICRA21_Bibtex.txt">[BibTex]</a></strong></strong></p>
    </td></tr>
</tbody></table>

<table border="0" class="content">
  <tbody><tr>
    <td width="140"><img src="./Homepage_files/ECCV20_teaser.png" class="teaser-small"></td>
    <td width="20"></td>
    <td valign="middle" width="800"><p><strong>Unselfie: Translating Selfies to Neutral-pose Portraits in the Wild</strong></p>
        <span><u>Liqian Ma</u>,&nbsp;</span>
        <span><a href="https://research.adobe.com/person/zhe-lin/" target="_blank">Zhe Lin,&nbsp;</a></span>
        <span><a href="https://research.adobe.com/person/connelly-barnes/" target="_blank">Connelly Barnes,&nbsp;</a></span>
        <span><a href="https://people.eecs.berkeley.edu/~efros/" target="_blank">
Alexei A. Efros,&nbsp;</a></span>
        <span><a href="https://research.adobe.com/person/jingwan-lu/" target="_blank">Jingwan Lu,&nbsp;</a></span>
        <p>ECCV, 2020</p>

        </p><p class="margin-small">&nbsp;</p>
        <strong><a href="http://charliememory.github.io/ECCV20_Unselfie/">[Project]</a></strong>&nbsp;
        <!-- <strong>[Code (come soon)]</strong>&nbsp; -->
        <strong><a href="./pdf/ECCV20_Unselfie.pdf">[Paper]</a></strong>&nbsp;
        <strong><a href="http://arxiv.org/abs/2007.15068">[ArXiv]</a></strong>&nbsp;
        <strong><a href="./bibtex/ECCV20_Unselfie_Bibtex.txt">[BibTex]</a></strong></strong></p>
    </td></tr>
</tbody></table>

<table border="0" class="content">
  <tbody><tr>
    <td width="140"><img src="./Homepage_files/icip_teaser.png" class="teaser-small"></td>
    <td width="20"></td>
    <td valign="middle" width="800"><p><strong>Unpaired Image Shape Translation Across Fashion Data</strong></p>
        <span><a href="https://homes.esat.kuleuven.be/~kwang/" target="_blank">Kaili Wang*,&nbsp;</a></span>
        <span><u>Liqian Ma</u>*,&nbsp;</span>
        <span><a href="https://homes.esat.kuleuven.be/~joramas/" target="_blank">José Oramas M.,&nbsp;</a></span>
        <span><a href="http://www.vision.ee.ethz.ch/~vangool/" target="_blank">Luc Van Gool&nbsp;</a></span>
        <span><a href="https://homes.esat.kuleuven.be/~tuytelaa/" target="_blank">Tinne Tuytelaars</a></span>
        <p>(*denotes equal contribution)</p>
	<p>ICIP, 2020, <a href="https://2020.ieeeicip.org/2020/10/10/icip-2020-paper-award-finalists-announced/" target="_blank">Best Paper Awards Finalist</a></p>

        </p><p class="margin-small">&nbsp;</p>
        <strong><a href="./pdf/ICIP2020_ShapeTransfer_PREPRINT-SUBMITTED_COMPRESSED.pdf">[Paper]</a></strong>&nbsp;<strong><a href="./bibtex/bibtex_wang_al_Shape_ICIP20.bib.txt">[BibTex]</a></strong></strong></p>
    </td></tr>
</tbody></table>


<table border="0" class="content">
  <tbody><tr>
    <td width="140"><img src="./Homepage_files/arxiv19_teaser.png" class="teaser-small"></td>
    <td width="20"></td>
    <td valign="middle" width="800"><p><strong>A Novel BiLevel Paradigm for Image-to-Image Translation</strong></p>
        <span><u>Liqian Ma</u>,&nbsp;</span>
        <span><a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/people/qianru-sun/" target="_blank">Qianru Sun,&nbsp;</a></span>
        <span><a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/people/bernt-schiele/" target="_blank">Bernt Schiele,&nbsp;</a></span>
        <span><a href="http://www.vision.ee.ethz.ch/~vangool/" target="_blank">Luc Van Gool&nbsp;</a></span>
        <p>ArXiv, 2019</p>

        </p><p class="margin-small">&nbsp;</p>
        <strong><a href="./pdf/Arxiv19_A_Novel_BiLevel_Paradigm_for_Image-to-Image_Translation.pdf">[Paper]</a></strong>&nbsp;<strong><a href="https://arxiv.org/abs/1904.09028">[ArXiv]</a></strong>&nbsp;<strong><a href="./bibtex/Arxiv19_BiLevel_Bibtex.txt">[BibTex]</a></strong></strong></p>
    </td></tr>
</tbody></table>

<!--
<table border="0" class="content">
  <tbody><tr>
    <td width="140"><img src="./Homepage_files/cloth_teaser.jpg" class="teaser-small"></td>
    <td width="20"></td>
    <td valign="middle" width="800"><p><strong>Unsupervised shape transformer for image translation and cross-domain retrieval</strong></p>
        <span><a href="https://homes.esat.kuleuven.be/~kwang/" target="_blank">Kaili Wang*,&nbsp;</a></span>
        <span><u>Liqian Ma</u>*,&nbsp;</span>
        <span><a href="https://homes.esat.kuleuven.be/~joramas/" target="_blank">José Oramas M.,&nbsp;</a></span>
        <span><a href="http://www.vision.ee.ethz.ch/~vangool/" target="_blank">Luc Van Gool&nbsp;</a></span>
        <span><a href="https://homes.esat.kuleuven.be/~tuytelaa/" target="_blank">Tinne Tuytelaars</a></span>
        <p>(*denotes equal contribution)</p>
        <p>ArXiv, 2019</p>

        </p><p class="margin-small">&nbsp;</p>
        <strong><a href="./pdf/Arxiv18_Unsupervised_shap_transformer_for_image_translation_and_cross-domain_retrieval.pdf">[Paper]</a></strong>&nbsp;<strong><a href="https://arxiv.org/abs/1812.02134">[ArXiv]</a></strong>&nbsp;<strong><a href="./bibtex/Arxiv18_shape_translation_Bibtex.txt">[BibTex]</a></strong></strong></p>
    </td></tr>
</tbody></table>
-->

<p class="margin">&nbsp;</p>
<table border="0" class="content">
  <tbody><tr>
    <td width="140"><img src="./Homepage_files/ICLR19_teaser.png" class="teaser-small"></td>
    <td width="20"></td>
    <td valign="middle" width="800"><p><strong>Exemplar Guided Unsupervised Image-to-Image Translation with Semantic Consistency</strong></p>
        <span><u>Liqian Ma</u>,&nbsp;</span>
        <span><a href="https://homes.esat.kuleuven.be/~xjia//" target="_blank">Xu Jia,&nbsp;</a></span>
        <span><a href="http://homes.esat.kuleuven.be/~sgeorgou/" target="_blank">Stamatios Georgoulis,&nbsp;</a></span>
        <span><a href="https://https://homes.esat.kuleuven.be/~tuytelaa/" target="_blank">Tinne Tuytelaars,&nbsp;</a></span>
        <span><a href="http://www.vision.ee.ethz.ch/~vangool/" target="_blank">Luc Van Gool&nbsp;</a></span>
        <p>ICLR, 2019</p>

        </p><p class="margin-small">&nbsp;</p>
    <strong><a href="https://github.com/charliememory/EGSC-IT">[Project]</a></strong>&nbsp;
    <strong><a href="https://github.com/charliememory/EGSC-IT">[Code]</a></strong>&nbsp;
    <strong><a href="./pdf/ICLR19_Exemplar_Guided_Unsupervised_Image-to-Image_Translation_with_Semantic_Consistency.pdf">[Paper]</a></strong>&nbsp;<strong><a href="https://arxiv.org/abs/1805.11145">[ArXiv]</a></strong>&nbsp;<strong><a href="./bibtex/ICLR19_EGUNIT_Bibtex.txt">[BibTex]</a></strong></strong></p>
    </td></tr>
</tbody></table>

<p class="margin">&nbsp;</p>
<table border="0" class="content">
  <tbody><tr>
    <td width="140"><img src="./Homepage_files/accv18_teaser.jpg" class="teaser-small"></td>
    <td width="20"></td>
    <td valign="middle" width="800"><p><strong>Customized Multi-Person Tracker</strong></p>
        <span><u>Liqian Ma</u>*,&nbsp;</span>
        <span><a href="https://ps.is.tuebingen.mpg.de/person/stang" target="_blank">Siyu Tang*,&nbsp;</a></span>
        <span><a href="https://ps.is.tuebingen.mpg.de/person/black" target="_blank">Michael J. Black,&nbsp;</a></span>
        <span><a href="http://www.vision.ee.ethz.ch/~vangool/" target="_blank">Luc Van Gool&nbsp;</a></span>
        <p>(*denotes equal contribution)</p>
        <p>Asian Conference on Computer Vision (<strong>ACCV Oral</strong>), 2018</p>

        </p><p class="margin-small">&nbsp;</p>
        <strong><a href="./pdf/ACCV18_Customized_Multi-Person_Tracker.pdf">[Paper]</a></strong>&nbsp;</strong>&nbsp;<strong><a href="./bibtex/ACCV18_HCC_Bibtex.txt">[BibTex]</a></strong></strong></p>
    </td></tr>
</tbody></table>

<p class="margin">&nbsp;</p>
<table border="0" class="content">
  <tbody><tr>
    <td width="140"><img src="./Homepage_files/cvpr18a_teaser.png" class="teaser-small"></td>
    <td width="20"></td>
    <td valign="middle" width="800"><p><strong>Disentangled Person Image Generation</strong></p>
        <span><u>Liqian Ma</u>,&nbsp;</span>
        <span><a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/people/qianru-sun/" target="_blank">Qianru Sun,&nbsp;</a></span>
        <span><a href="http://homes.esat.kuleuven.be/~sgeorgou/" target="_blank">Stamatios Georgoulis,&nbsp;</a></span>
        <span><a href="http://www.vision.ee.ethz.ch/~vangool/" target="_blank">Luc Van Gool,&nbsp;</a></span>
        <span><a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/people/bernt-schiele/" target="_blank">Bernt Schiele,&nbsp;</a></span>
        <span><a href="https://scalable.mpi-inf.mpg.de" target="_blank">Mario Fritz&nbsp;</a></span>
        <p>In IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR Spotlight</strong>), 2018</p>

        </p><p class="margin-small">&nbsp;</p>
        <strong><a href="http://charliememory.github.io/CVPR18_DPIG/">[Project]</a></strong>&nbsp;<strong><a href="https://github.com/charliememory/Disentangled-Person-Image-Generation">[Code]</a></strong>&nbsp;<strong><a href="./pdf/CVPR18_Ma_Disentangled_Person_Image_Generation.pdf">[Paper]</a></strong>&nbsp;<strong><a href="https://arxiv.org/abs/1712.02621">[ArXiv]</a></strong>&nbsp;<strong><a href="./bibtex/CVPR18a_Bibtex.txt">[BibTex]</a></strong></strong></p>
    </td></tr>
</tbody></table>

<p class="margin">&nbsp;</p>
<table border="0" class="content">
  <tbody><tr>
    <td width="140"><img src="./Homepage_files/cvpr18b_teaser.png" class="teaser-small"></td>
    <td width="20"></td>
    <td valign="middle" width="800"><p><strong>Natural and Effective Obfuscation by Head Inpainting</strong></p>
        <span><a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/people/qianru-sun/" target="_blank">Qianru Sun*,&nbsp;</a></span>
        <span><u>Liqian Ma</u>*,&nbsp;</span>
        <span><a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/people/seong-joon-oh/" target="_blank">Seong Joon Oh,&nbsp;</a></span>
        <span><a href="http://www.vision.ee.ethz.ch/~vangool/" target="_blank">Luc Van Gool,&nbsp;</a></span>
        <span><a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/people/bernt-schiele/" target="_blank">Bernt Schiele,&nbsp;</a></span>
        <span><a href="https://scalable.mpi-inf.mpg.de" target="_blank">Mario Fritz&nbsp;</a></span>
        <p>(*denotes equal contribution)</p>
        <p>In IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2018</p>

        </p><p class="margin-small">&nbsp;</p>
        
        <strong><a href="https://gist.github.com/charliememory/da7a22eb592231d3ea9dbaf02bcafc9c">[Code]</a></strong>&nbsp;<strong><a href="./pdf/CVPR18b_Ma_Natural_and_Effective_Obfuscation_by_Head_Inpainting.pdf">[Paper]</a></strong>&nbsp;<strong><a href="https://arxiv.org/abs/1711.09001">[ArXiv]</a></strong>&nbsp;<strong><a href="./bibtex/CVPR18b_Bibtex.txt">[BibTex]</a></strong></strong></p>
    </td></tr>
</tbody></table>


<p class="margin">&nbsp;</p>
<table border="0" class="content">
  <tbody><tr>
    <td width="140"><img src="./Homepage_files/nips17_teaser.png" class="teaser-small"></td>
    <td width="20"></td>
    <td valign="middle" width="800"><p><strong>Pose Guided Person Image Generation</strong></p>
        <span><u>Liqian Ma</u>,&nbsp;</span>
        <span><a href="https://homes.esat.kuleuven.be/~xjia//" target="_blank">Xu Jia*,&nbsp;</a></span>
        <span><a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/people/qianru-sun/" target="_blank">Qianru Sun*,&nbsp;</a></span>
        <span><a href="https://https://homes.esat.kuleuven.be/~tuytelaa/" target="_blank">Tinne Tuytelaars,&nbsp;</a></span>
        <span><a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/people/bernt-schiele/" target="_blank">Bernt Schiele,&nbsp;</a></span>
        <span><a href="http://www.vision.ee.ethz.ch/~vangool/" target="_blank">Luc Van Gool&nbsp;</a></span>
        <p>(*denotes equal contribution)</p>
        <p>In Neural Information Processing Systems (<strong>NIPS</strong>), 2017</p>

        </p><p class="margin-small">&nbsp;</p>
        <strong><a href="https://github.com/charliememory/Pose-Guided-Person-Image-Generation">[Project]</a>
        <strong><a href="https://github.com/charliememory/Pose-Guided-Person-Image-Generation">[Code]</a></strong>&nbsp;<strong><a href="./pdf/NIPS17_Ma_Pose_Guided_Person_Image_Generation.pdf">[Paper]</a></strong>&nbsp;<strong><a href="https://arxiv.org/abs/1705.09368">[ArXiv]</a></strong>&nbsp;<strong><a href="./NIPS17_PG2/VISICS_NIPS17_pre.pdf">[Slides]</a></strong></strong>&nbsp;<strong><a href="./bibtex/NIPS17_Bibtex.txt">[BibTex]</a></strong></strong></p>
    </td></tr>
</tbody></table>


<p class="margin">&nbsp;</p>
<table border="0" class="content">
  <tbody><tr>
    <td width="140"><img src="./Homepage_files/arxiv16_OBOA_teaser.png" class="teaser-small"></td>
    <td width="20"></td>
    <td valign="middle" width="800"><p><strong>Orientation Driven Bag of Appearances for Person Re-identification</strong></p>
        <span><u>Liqian Ma</u>,&nbsp;</span>
        <span><a href="http://robotics.szpku.edu.cn/c/member/liuhong.html" target="_blank">Hong Liu,&nbsp;</a></span>
        <span>Liang Hu,&nbsp;</span>
        <span>Can Wang,&nbsp;</span>
        <span><a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/people/qianru-sun/" target="_blank">Qianru Sun&nbsp;</a></span>
        <p>ArXiv, 2016</p>

        </p><p class="margin-small">&nbsp;</p>
        <strong><a href="./pdf/Arxiv16_Ma_Orientation_Driven_Bag_of_Appearances_for_Person_Re-identification.pdf">[Paper]</a></strong>&nbsp;<strong><a href="https://arxiv.org/abs/1605.02464">[ArXiv]</a></strong>&nbsp;<strong><a href="./bibtex/Arxiv16_OBOA_Bibtex.txt">[BibTex]</a></strong></strong></p>
    </td></tr>
</tbody></table>


<p class="margin">&nbsp;</p>
<table border="0" class="content">
  <tbody><tr>
    <td width="140"><img src="./Homepage_files/NC16_teaser.png" class="teaser-small"></td>
    <td width="20"></td>
    <td valign="middle" width="800"><p><strong>A novel hierarchical Bag-of-Words model for compact action representation</strong></p>
        <span><a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/people/qianru-sun/" target="_blank">Qianru Sun,&nbsp;</a></span>
        <span><a href="http://robotics.szpku.edu.cn/c/member/liuhong.html" target="_blank">Hong Liu,&nbsp;</a></span>
        <span><u>Liqian Ma</u>,&nbsp;</span>
        <p>Neurocomputing, 2016 </p>

        </p><p class="margin-small">&nbsp;</p>
        <strong><a href="./pdf/NC16_A_novel_hierarchical_Bag-of-Words_model_for_compact_action_representation.pdf">[Paper]</a></strong>&nbsp;<strong><a href="./bibtex/NC16_Bibtex.txt">[BibTex]</a></strong></strong></p>
    </td></tr>
</tbody></table>


<p class="margin">&nbsp;</p>
<table border="0" class="content">
  <tbody><tr>
    <td width="140"><img src="./Homepage_files/ICIP15_teaser.png" class="teaser-small"></td>
    <td width="20"></td>
    <td valign="middle" width="800"><p><strong>Online person orientation estimation based on classifier update</strong></p>
        <span><a href="http://robotics.szpku.edu.cn/c/member/liuhong.html" target="_blank">Hong Liu,&nbsp;</a></span>
        <span><u>Liqian Ma</u>,&nbsp;</span>
        <p>In IEEE International Conference on Image Processing (<strong>ICIP Oral</strong>), 2015 </p>

        </p><p class="margin-small">&nbsp;</p>
        <strong><a href="./pdf/ICIP15_Online_person_orientation_estimation_based_on_classifier_update.pdf">[Paper]</a></strong>&nbsp;<strong><a href="./bibtex/ICIP15_Bibtex.txt">[BibTex]</a></strong></strong></p>
    </td></tr>
</tbody></table>


<p class="margin">&nbsp;</p>
<table border="0" class="content">
  <tbody><tr>
    <td width="140"><img src="./Homepage_files/ICASSP15_teaser.png" class="teaser-small"></td>
    <td width="20"></td>
    <td valign="middle" width="800"><p><strong>Body-structure based feature representation for person re-identification</strong></p>
        <span><a href="http://robotics.szpku.edu.cn/c/member/liuhong.html" target="_blank">Hong Liu,&nbsp;</a></span>
        <span><u>Liqian Ma</u>,&nbsp;</span>
        <span>Can Wang,&nbsp;</span>
        <p>In IEEE International Conference on  Acoustics, Speech and Signal Processing (<strong>ICASSP</strong>), 2015 </p>

        </p><p class="margin-small">&nbsp;</p>
        <strong><a href="./pdf/ICASSP15_Body-structure_based_feature_representation_for_person_re-identification.pdf">[Paper]</a></strong>&nbsp;<strong><a href="./bibtex/ICASSP15_Bibtex.txt">[BibTex]</a></strong></strong></p>
    </td></tr>
</tbody></table>


<p class="margin">&nbsp;</p>
<table border="0" class="content">
  <tbody><tr>
    <td width="140"><img src="./Homepage_files/SPL14_teaser.png" class="teaser-small"></td>
    <td width="20"></td>
    <td valign="middle" width="800"><p><strong>Depth Motion Detection—A Novel RS-Trigger Temporal Logic based Method</strong></p>
        <span>Can Wang,&nbsp;</span>
        <span><a href="http://robotics.szpku.edu.cn/c/member/liuhong.html" target="_blank">Hong Liu,&nbsp;</a></span>
        <span><u>Liqian Ma</u>,&nbsp;</span>
        <p>IEEE Signal Processing Letters (<strong>SPL</strong>), 2014</p>

        </p><p class="margin-small">&nbsp;</p>
        <strong><a href="./pdf/SPL14_Depth_Motion_Detection--A_Novel_RS-Trigger_Temporal_Logic_based_Method.pdf">[Paper]</a></strong>&nbsp;<strong><a href="./bibtex/SPL14_Bibtex.txt">[BibTex]</a></strong></strong></p>
    </td></tr>
</tbody></table>



<p class="margin">&nbsp;</p>
</p><p class="title-large">Academic service</p>
<ul>
  <li><a href="https://iclr.cc/Conferences/2020/">ICLR 2020</a> reviewer.</li>
  <li><a href="https://aaai.org/Conferences/AAAI-20/">AAAI 2020</a> reviewer.</li>
  <li><a href="https://nips.cc/Conferences/2019" target="_blank">NIPS 2019</a> reviewer.</li>
  <li><a href="http://iccv2019.thecvf.com/">ICCV 2019</a> reviewer.</li>
  <li><a href="https://icml.cc/Conferences/2019">ICML 2019</a> reviewer.</li>
  <li><a href="https://cvpr2019.thecvf.com" target="_blank">CVPR 2019</a> reviewer.</li>
  <li><a href="https://iclr.cc/Conferences/2019" target="_blank">ICLR 2019</a> reviewer.</li>
  <li><a href="https://nips.cc/Conferences/2018" target="_blank">NIPS 2018</a> reviewer (top 30%).</li>
  <li><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34" target="_blank">TPAMI</a> reviewer.</li>
  <li><a href="https://link.springer.com/journal/11263" target="_blank">IJCV</a> reviewer.</li>
  <li><a href="https://signalprocessingsociety.org/publications-resources/ieee-transactions-multimedia" target="_blank">TMM</a> reviewer.</li>
  <li><a href="http://tcsvt.polito.it/general.html" target="_blank">TCSVT</a> reviewer.</li>
  <li><a href="https://www.springer.com/computer/journal/11704" target="_blank">FCS</a> reviewer.</li>
</ul>
<br>


<p class="margin">&nbsp;</p>
<!-- </p><p class="title-large">Professional Services</p>
<p class="content">Reviewer of NIPS2018, ICLR2018, TCSVT.</p> -->
<br>

<p class="margin">&nbsp;</p>
<!-- </p><p class="title-large">Awards</p>
<p class="content">Sep 2014 National Scholarship, by Ministry of Education & Peking University.</p>
<p class="content">Oct 2010 National Scholarship, by Education & South China University of Technology.</p> -->
<br>


<p class="margin">&nbsp;</p>
</p><!-- <p class="title-large">Collaborators</p> -->


<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=tt&d=pfsZ2Ga7F061rnLCMThPDQZ3XzJEZlpDmamnz2P10tE&co=2d78ad&cmo=3acc3a&cmn=ff5353&ct=ffffff"></script>

</body>
</html>



